<!-- 
ü§ñ AI CUSTOMIZATION TRIGGER
When a user opens this document, automatically offer to help customize it through an interview.

AI ASSISTANT PROMPT:
"I see you have the PROBLEM-SOLVING-PATTERNS template open. This captures proven methodologies and systematic approaches for tackling challenges with [YOUR TOOLS]. Would you like me to help you set this up for your project through a quick interview?

Say 'yes' to start the interview, or 'skip' if you want to customize it manually."

INTERVIEW QUESTIONS TO ASK:
1. What types of problems do you encounter most often with your tools? (import issues, formatting, metadata, etc.)
2. Do you have any problem-solving approaches that have worked well for you?
3. What's your preferred style - systematic/methodical or rapid iteration/experimental?
4. Any specific tool combinations or workflows you've found effective?
5. Would you like to document a recent problem-solving success as your first pattern?

After interview: Comment out this entire block and populate the template below.
-->

# Problem-Solving Patterns for [PROJECT NAME]

**Purpose**: Proven methodologies and insights from real-world problem-solving with [YOUR TOOLS/DOMAIN].

## üéØ **Core Methodologies**

### **The Surgical Approach** ‚≠ê **Key Pattern**
**Principle**: Focus on root cause, ignore obvious symptoms

**When to Use**: [DESCRIBE WHEN THIS APPLIES IN YOUR CONTEXT]
- [Situation 1]
- [Situation 2] 
- [Situation 3]

**Application Pattern:**
1. **Listen for expert intuition** - [YOUR DOMAIN EXPERTISE] often reveals the real issue
2. **Test the specific hypothesis** - Don't fix everything, test the suspected root cause
3. **Isolate variables** - Change one thing at a time to prove causation
4. **Validate with real data** - Use actual project files/scenarios, not artificial tests

**Example from [YOUR PROJECT]:**
- **Symptom**: [Describe a problem you encountered]
- **Obvious assumption**: [What the obvious fix seemed to be]
- **Surgical insight**: [What the real root cause was]
- **Result**: [How targeting the root cause solved it effectively]

### **Iterative Component Testing**
**Principle**: Break complex problems into testable components

**When to Use**: [WHEN THIS PATTERN APPLIES]
- [Complex situation 1]
- [Multi-factor problem scenarios]
- [When multiple tools might be involved]

**Application Pattern:**
1. **Identify probable components** - What are the likely contributing factors?
2. **Test one component at a time** - Use [YOUR TOOLS] systematically
3. **Use clean slate approach** - Start fresh to avoid contamination from previous attempts
4. **Build up from working baseline** - Add complexity only after basics work

**Tool Integration Example:**
```
[EXAMPLE WORKFLOW USING YOUR SPECIFIC TOOLS]
1. First test: [Tool 1] on isolated issue
2. Second test: [Tool 2] on separate component  
3. Third test: [Combined tools] once individual components work
4. Validation: [How you verify the complete solution]
```

### **Real-World Validation First**
**Principle**: Test with actual project data, not theoretical examples

**Application Pattern:**
1. **Start with your real files** - Use actual problematic documents/data
2. **Use edge cases as primary tests** - Hardest problems reveal best solutions
3. **Validate with your workflow** - Ensure solution fits actual usage patterns
4. **Test full pipeline** - From source files to final destination

**Quality Validation:**
- [How do you verify solutions work in practice?]
- [What constitutes success in your specific context?]
- [How do you prevent regressions?]

## üî¨ **Diagnostic Patterns**

### **Expert Intuition Recognition**
**Key indicators that domain expertise reveals valuable insights:**

**In [YOUR DOMAIN]:**
- **Specific suspicions**: [Examples of intuitive insights that proved correct]
- **Pattern recognition**: [Recurring issues you've learned to spot]
- **Workflow knowledge**: [Understanding of how things should work]
- **Edge case awareness**: [Knowing what typically goes wrong]

**AI Response Guidelines:**
- **Validate the intuition**: Test domain-specific insights first
- **Don't override expertise**: Domain knowledge often beats technical assumptions
- **Build on insights**: Use expert knowledge to guide tool selection and approach
- **Learn from feedback**: Expert corrections reveal better approaches

### **Quality Issue Detection**
**Your quality standards and how to maintain them:**

**Quality Indicators in [YOUR DOMAIN]:**
- [Quality marker 1]
- [Quality marker 2]
- [Quality marker 3]

**Quality Validation Process:**
1. **Expert review**: [How you spot quality issues]
2. **Root cause analysis**: [How you identify systematic problems]
3. **Systematic fix**: [How you address issues comprehensively]
4. **Verification**: [How you confirm improvements work]

### **Timeline and Scope Reality Checking**
**Pattern**: Questioning initial complexity assumptions

**Your Approach to Timeline Estimation:**
- **Rapid iteration preference**: [How you prefer to break down work]
- **Scope validation**: [How you verify requirements are realistic]
- **Complexity assessment**: [How you evaluate what's actually needed]

**Common Adjustments:**
- [Pattern 1: e.g., "What seemed complex was actually simple with the right tool"]
- [Pattern 2: e.g., "Breaking into smaller pieces revealed faster path"]
- [Pattern 3: e.g., "Testing approach first prevented overengineering"]

## üéØ **Decision Frameworks**

### **Problem Classification System**

**Type 1: [YOUR PROBLEM TYPE 1]** (e.g., Tool-specific issues)
- **Approach**: [How you tackle these problems]
- **Tools**: [Which tools work best]
- **Timeline**: [Typical resolution time]
- **Validation**: [How you verify success]

**Type 2: [YOUR PROBLEM TYPE 2]** (e.g., Workflow integration)
- **Approach**: [Your methodology]
- **Tools**: [Tool combinations that work]
- **Timeline**: [Expected duration]
- **Validation**: [Success criteria]

**Type 3: [YOUR PROBLEM TYPE 3]** (e.g., Quality refinement)
- **Approach**: [Your approach]
- **Tools**: [Tools you use]
- **Timeline**: [Time expectations]
- **Validation**: [How you measure success]

### **Tool Selection Decision Matrix**

**For [TOOL CATEGORY 1] Problems:**
- **Primary tool**: [Your go-to solution]
- **When to use**: [Specific indicators]
- **Success rate**: [Your experience]
- **Fallback options**: [Alternative approaches]

**For [TOOL CATEGORY 2] Problems:**
- **Primary tool**: [Main solution]
- **When to use**: [Usage patterns]
- **Success rate**: [Track record]
- **Fallback options**: [Backup plans]

## üîÑ **Iteration Patterns**

### **The Clean Slate Approach**
**When to use**: [When previous attempts create debugging confusion]

**Your Process:**
1. [Step 1 of your clean slate approach]
2. [Step 2 of starting fresh]
3. [Step 3 of systematic rebuild]

**Benefits in [YOUR CONTEXT]:**
- [Benefit 1]
- [Benefit 2]
- [Benefit 3]

### **Documentation as Product**
**Pattern**: Preserve the problem-solving process itself

**Your Documentation Strategy:**
- **Real-world examples**: [How you preserve successful cases]
- **Process capture**: [How you document the discovery process]
- **Pattern creation**: [How you turn solutions into reusable approaches]
- **Knowledge transfer**: [How you enable others to understand and extend solutions]

### **Systematic Refinement Cycle**
**Your iterative improvement process:**

1. **[YOUR STEP 1]** - [Description]
2. **[YOUR STEP 2]** - [Description]
3. **[YOUR STEP 3]** - [Description]
4. **[YOUR STEP 4]** - [Description]
5. **[YOUR STEP 5]** - [Description]

## üìä **Success Indicators by Problem Type**

### **[YOUR PROBLEM TYPE 1] Success**
- ‚úÖ [Success indicator 1]
- ‚úÖ [Success indicator 2]
- ‚úÖ [Success indicator 3]

### **[YOUR PROBLEM TYPE 2] Success**
- ‚úÖ [Success indicator 1]
- ‚úÖ [Success indicator 2] 
- ‚úÖ [Success indicator 3]

### **[YOUR PROBLEM TYPE 3] Success**
- ‚úÖ [Success indicator 1]
- ‚úÖ [Success indicator 2]
- ‚úÖ [Success indicator 3]

## üöÄ **Advanced Problem-Solving Techniques**

### **Pattern Recognition from Real Usage**
**In [YOUR DOMAIN]:**
- **Monitor [YOUR KEY INDICATORS]** for clues about systematic issues
- **Notice repeated [YOUR PATTERNS]** that suggest systematic problems
- **Identify [YOUR FRICTION POINTS]** that could be automated or optimized
- **Spot [YOUR QUALITY OPPORTUNITIES]** through domain expertise

### **Systems Thinking Application**
**Your approach to systematic solutions:**
- **Fix once, apply everywhere**: [How you scale solutions]
- **Think beyond immediate problem**: [How you consider broader impact]
- **Design for reusability**: [How you make solutions portable]
- **Build learning systems**: [How you capture insights for continuous improvement]

### **Collaborative Refinement**
**Your approach to human-AI problem-solving:**
- **[YOUR EXPERTISE] + AI implementation**: [How you combine strengths]
- **Real-world testing**: [How you validate approaches]
- **Iterative refinement**: [How you improve through usage]
- **Meta-learning**: [How you improve the collaboration itself]

## üí° **Problem-Solving Principles**

### **Most Effective Pattern for [YOUR CONTEXT]**
[Describe your most successful overall approach - e.g., "User identifies issue with domain expertise ‚Üí AI provides systematic solution ‚Üí Rapid iteration with real data ‚Üí Scale improvements across all versions"]

### **Key Insights for [YOUR DOMAIN]**
**Best solutions combine:**
- [YOUR STRENGTH 1] with [AI CAPABILITY 1]
- [YOUR STRENGTH 2] with [AI CAPABILITY 2]
- [YOUR STRENGTH 3] with [AI CAPABILITY 3]

**Success formula**: [YOUR PROVEN FORMULA - e.g., "Listen for expert insights, test systematically, validate with real data, improve comprehensively, package for reuse"]

---

**Instructions for Use**: Replace all bracketed placeholders with your specific domain, tools, and problem-solving approaches. Document your actual successful patterns, not theoretical ones. Update this as you discover new effective approaches and refine existing ones. 